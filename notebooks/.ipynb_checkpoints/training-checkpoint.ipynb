{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing modules...\n",
      "generating model\n",
      "model created\n",
      "CPU times: user 697 ms, sys: 21.2 ms, total: 718 ms\n",
      "Wall time: 737 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('importing modules...')\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import sys, os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "from random import randint\n",
    "sys.path.append(os.pardir)\n",
    "from ssd_vgg16 import SSD\n",
    "\n",
    "annotations = {'Car':100,'Truck':100,'Pedestrian':100,'stop_sign':100}\n",
    "fmap = {'conv4_3_norm': [1.0, 2.0, 1.0/2.0],\n",
    "        'fc7': [1.0, 2.0, 3.0, 1.0/2.0, 1.0/3.0],\n",
    "        'conv6_2': [1.0, 2.0, 3.0, 1.0/2.0, 1.0/3.0],\n",
    "        'conv7_2': [1.0, 2.0, 3.0, 1.0/2.0, 1.0/3.0],\n",
    "        'conv8_2': [1.0, 2.0, 1.0/2.0],\n",
    "        'conv9_2': [1.0, 2.0, 1.0/2.0]}\n",
    "\n",
    "input_shape = (300, 300, 3)\n",
    "NUM_CLASSES = len(annotations) + 1\n",
    "\n",
    "print(\"generating model\")\n",
    "model = SSD((300,300,3), num_classes=NUM_CLASSES, fmap=fmap)\n",
    "print(\"model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.merge.Concatenate at 0x11bc26b70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make data and prepare for training\n",
    "img_folderpath = 'data/img/'\n",
    "csv_filepath = 'data/alldata.csv'\n",
    "\n",
    "print('making data...')\n",
    "csv = pd.read_csv(csv_filepath, index_col=0)\n",
    "\n",
    "print('picking files randomly...')\n",
    "use_imgfile_names = pd.Series()\n",
    "for i in range(len(annotation)):\n",
    "    tmp = csv.filename[csv.object.isin([[x for x in annotation.keys()][i]])].sample(n=[v for v in annotation.values()][i])\n",
    "    use_imgfile_names = use_imgfile_names.append(tmp)\n",
    "\n",
    "print('reading image files...')\n",
    "annotation_list = []\n",
    "imagesize_list = []\n",
    "for name in use_imgfile_names:\n",
    "    try:\n",
    "        im = Image.open(img_folderpath + name)\n",
    "        t = list(im.size)\n",
    "        t.insert(0,name)\n",
    "        imagesize_list.append(t)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "print('concatenating...')\n",
    "csv = csv[csv.filename.isin(use_imgfile_names) & csv.object.isin(annotation.keys())]\n",
    "imgsize = pd.DataFrame(imagesize_list).rename(index=str, columns={0:'filename', 1:'w', 2:'h'})\n",
    "df = pd.merge(csv, imgsize)\n",
    "df.xmin /= df.w\n",
    "df.ymin /= df.h\n",
    "df.xmax /= df.w\n",
    "df.ymax /= df.h\n",
    "df['bounding_box'] = df[['xmin','ymin','xmax','ymax']].values.tolist()\n",
    "one_hot_category = []\n",
    "for i,row in df.iterrows():\n",
    "    one_hot = [0] * len(annotation)\n",
    "    pos = [k for k,val in enumerate(annotation) if val == row.object][0]\n",
    "    one_hot[pos] = 1\n",
    "    one_hot_category.append(one_hot)\n",
    "df['one_hot_category'] = one_hot_category\n",
    "df['one_hot_vector'] = df.bounding_box + df.one_hot_category\n",
    "df = pd.DataFrame(df.groupby(['filename','w','h']).one_hot_vector.apply(list)).reset_index()\n",
    "gt = dict()\n",
    "for i,row in df.iterrows():\n",
    "    gt[row.filename] = np.array(row.one_hot_vector)\n",
    "\n",
    "print('generating train / validation files...')\n",
    "keys = sorted(gt.keys())\n",
    "num_train = int(round(0.8 * len(keys)))\n",
    "train_keys = keys[:num_train]\n",
    "val_keys = keys[num_train:]\n",
    "num_val = len(val_keys)\n",
    "\n",
    "inputs = []\n",
    "img_path = img_folderpath + sorted(val_keys)[0]\n",
    "img = image.load_img(img_path, target_size=(input_shape[1],input_shape[0]))\n",
    "img = image.img_to_array(img)\n",
    "inputs.append(img.copy())\n",
    "inputs = preprocess_input(np.array(inputs))\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample weights (optional)\n",
    "weights_filepath = 'VGG_coco_SSD_300x300_iter_400000_subsampled_4_classes.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=[0.07, 0.15, 0.33, 0.51, 0.69, 0.87, 1.05], # MS_COCO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
